import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
import time
import numpy as np

print("="*40)
print("ğŸ‹ï¸  STARTING CLASSICAL BASELINE TRAINING")
print("="*40)

# 1. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¬Ù‡Ø§Ø² (GPU Check)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸš€ Training on: {torch.cuda.get_device_name(0)}")

# 2. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
print("ğŸ“‚ Loading processed data...")
data = torch.load('processed_data.pt')
X_train, y_train = data['X_train'].to(device), data['y_train'].to(device)
X_test, y_test = data['X_test'].to(device), data['y_test'].to(device)

# Ø§Ø³ØªØ®Ø¯Ø§Ù… DataLoader Ù„Ù„Ø³Ø±Ø¹Ø© (Batching)
# Batch Size = 1024 (ÙƒØ¨ÙŠØ± Ù„Ø£Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø© 24GB ØªØ³Ù…Ø­ Ø¨Ø°Ù„Ùƒ Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„ØªØ¯Ø±ÙŠØ¨)
train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=1024, shuffle=False)
test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=1024, shuffle=False)

# 3. ØªØµÙ…ÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (Classical LSTM)
class ClassicalLSTM(nn.Module):
    def __init__(self, input_size=1, hidden_size=50, output_size=1):
        super(ClassicalLSTM, self).__init__()
        # Ø·Ø¨Ù‚Ø© LSTM ØªØ³ØªÙ„Ù… Ø§Ù„ØªØ³Ù„Ø³Ù„ Ø§Ù„Ø²Ù…Ù†ÙŠ
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        # Ø·Ø¨Ù‚Ø© Ø®Ø·ÙŠØ© Ù„Ù„ØªÙˆÙ‚Ø¹ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
        self.fc = nn.Linear(hidden_size, output_size)
        
    def forward(self, x):
        # x shape: (batch, seq_len, features)
        out, _ = self.lstm(x)
        # Ù†Ø£Ø®Ø° Ø¢Ø®Ø± Ø®Ø·ÙˆØ© Ø²Ù…Ù†ÙŠØ© ÙÙ‚Ø· (Many-to-One)
        out = out[:, -1, :]
        out = self.fc(out)
        return out

model = ClassicalLSTM().to(device)
criterion = nn.MSELoss() # Ù…Ù‚ÙŠØ§Ø³ Ø§Ù„Ø®Ø·Ø£ (Mean Squared Error)
optimizer = optim.Adam(model.parameters(), lr=0.001)

print("-" * 30)
print("ğŸ—ï¸  Model Architecture Created (Standard LSTM)")
print(f"ğŸ“Š Total Parameters: {sum(p.numel() for p in model.parameters())}")
print("-" * 30)

# 4. Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (Training Loop)
EPOCHS = 50
train_losses = []
test_losses = []

start_time = time.time()

for epoch in range(EPOCHS):
    model.train()
    running_loss = 0.0
    for X_batch, y_batch in train_loader:
        optimizer.zero_grad()
        predictions = model(X_batch)
        loss = criterion(predictions, y_batch.unsqueeze(1)) # ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø´ÙƒÙ„ Ù„ÙŠØªØ·Ø§Ø¨Ù‚
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
    
    avg_train_loss = running_loss / len(train_loader)
    train_losses.append(avg_train_loss)
    
    # Ø§Ù„ØªÙ‚ÙŠÙŠÙ… (Validation)
    model.eval()
    with torch.no_grad():
        test_loss = 0.0
        for X_val, y_val in test_loader:
            preds = model(X_val)
            loss = criterion(preds, y_val.unsqueeze(1))
            test_loss += loss.item()
        avg_test_loss = test_loss / len(test_loader)
        test_losses.append(avg_test_loss)
    
    if (epoch+1) % 5 == 0:
        print(f"Epoch [{epoch+1}/{EPOCHS}] | Train Loss: {avg_train_loss:.6f} | Test Loss: {avg_test_loss:.6f}")

total_time = time.time() - start_time
print("="*40)
print("ğŸ TRAINING COMPLETE!")
print(f"â±ï¸  Total Time: {total_time:.2f} seconds")
print(f"ğŸ“‰ Final Test MSE: {test_losses[-1]:.6f}")
print("="*40)

# Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ù„Ø§Ø­Ù‚Ø§Ù‹
torch.save(model.state_dict(), 'baseline_model.pth')
np.save('baseline_metrics.npy', {'train': train_losses, 'test': test_losses, 'time': total_time})